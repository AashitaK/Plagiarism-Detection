{
  "cells": [
    {
      "metadata": {
        "_uuid": "1c5940368abe29a5bd29dcb8181a12ca2406765a"
      },
      "cell_type": "markdown",
      "source": "# Plagiarism Detection\n### Dataset\nThe dataset used here is a small sample taken from the [International Competition on Plagiarism Detection PAN 2010](https://www.uni-weimar.de/en/media/chairs/computer-science-department/webis/data/corpus-pan-pc-10/). It consists of two kinds of files - suspicious documents and source documents. The dataset also contains information about whether a suspicious documents is plagiarized or not from the source documents. \n\n### Model overview\nWe have built a classifier below to classify the suspicious documents as plagiarized or not. This is done in broadly two steps:\n1. First we derive four distinct similarity measures for each suspicious document. Three of the measures are derived from the overlapping of the **trigrams** of the text - Jaccard similarity coefficient, containment measure and the longest common sequence whereas the last one is derived from the closeness of the vectors in Latent Semantic Analysis (LSA). \n2. Next we use these measures as the features for the suspicious documents to train a logistic regression classifier on them. \n\n### Rationale\nPlagiarism comes in many forms. The first three of the similarity measures coming from the trigrams is targeted to catch the copying of the text from the source documents whereas the last measure related LSA attempts to catch the restructuring, revising and paraphrasing of the original text. \n\nThe model is designed after reviewing the literature and playing with various ideas such as in terms of text preprocessing. Many ideas, such as the first three similarity measures derived from trigrams, are borrowed from the paper [Using Natural Language Processing for Automatic Detection of Plagiarism](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.458.9440&rep=rep1&type=pdf)  by Miranda Chong, et. al. For the text pre-processing, the detailed analysis from the paper  [The Influence of Text Pre-processing on Plagiarism Detection](https://pdfs.semanticscholar.org/a47c/1a35e2858da1eb82077b572e538a7b0b7b2d.pdf) is taken into account while building the model. The figure 2 in the paper clearly depicts the impact of the most commonly used text preprocessing methods. The following decisions are made for text preprocessing:\n1. Stopwords are not removed since they seem to play a role in detecting overlapping in the trigrams\n2. Sentence segmentation is avoided since the plagiarism seem to span across consecutive sentences, hence the trigrams linking the sentences are useful as well.\n3. Numbers, colons, plus are removed whereas punctuations are kept.\n4. Lowercase is used throughout the text.\n5. Lemmatization is used but only for deriving the last feature (or measure) using LSA.\n6. POS-tagging might prove to be useful but the current model does not use it\n\n\n### Tools used: re (regular expressions), numpy, pandas, nltk, sklearn\n'\n### Ideas/Future work\n* Text preprocessing: POS-tagging, synonymy recognition, etc.\n* Using bigrams, 4-grams and/or 5-grams along with (or instead of) trigrams\n* Deriving more features for the classifier using more similarity measures or other approaches\n* Tuning the parameters for the logistic regression classifier\n* Modifying/optimizing the code for large-scale data\n\n### Challenges\n* Multi-source plagiarism:  When each suspicious documents is compared with all source documents to calculate the similarity meaures, the highest scores are considered. Thus, multi-source plagiarism  is not taken into account. This can be fixed by instead considering the average of the top 3 scores while calculating the similarity measures.\n* Language translation: Our model does not cover the plagiarism caused by translating the original text in another language\n* Paraphrasing is only partially addressed.\n* Model so far has a serious flaw that it might also flag a document that has properly quoted from the original source along with giving correct reference. This can be fixed by looking for the quotations at the very beginning. Check that the source for each quotation is properly attributed and then removing the quote along with the reference text.  \n\n"
    },
    {
      "metadata": {
        "_uuid": "155690268348cf1a611cb783962cb76453266664"
      },
      "cell_type": "markdown",
      "source": "We start by importing the relevant modules."
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np \nimport pandas as pd \nimport re\n\nimport os\npath = \"../input/\" # Update path\nprint(\"Files:\")\nprint(os.listdir(path))\n\nimport nltk\nfrom nltk import trigrams, word_tokenize\nfrom nltk.stem import WordNetLemmatizer \n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.linear_model import LogisticRegression",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Files:\n['suspicious-document00029.txt', 'suspicious-document00031.txt', 'source-document01651.txt', 'source-document01919.txt', 'source-document02707.txt', 'source-document01718.txt', 'suspicious-document00007.txt', 'suspicious-document00025.txt', 'source-document01328.txt', 'source-document07522.txt', 'source-document01999.txt', 'suspicious-document00010.txt', 'suspicious-document00014.txt', 'source-document03861.txt', 'suspicious-document00003.txt', 'source-document04426.txt', 'suspicious-document00028.txt', 'suspicious-document00021.txt', 'source-document01595.txt', 'suspicious-document00013.txt', 'suspicious-document00004.txt', 'source-document01600.txt', 'suspicious-document00009.txt', 'source-document00893.txt', 'suspicious-document00033.txt', 'suspicious-document00019.txt', 'suspicious-document00011.txt', 'suspicious-document00027.txt', 'suspicious-document00015.txt', 'source-document05310.txt', 'source-document04279.txt', 'source-document03029.txt', 'suspicious-document00005.txt', 'suspicious-document00030.txt', 'suspicious-document00002.txt', 'suspicious-document00026.txt', 'suspicious-document00012.txt', 'suspicious-document00020.txt', 'source-document04430.txt', 'suspicious-document00023.txt', 'suspicious-document00018.txt', 'suspicious-document00024.txt', 'suspicious-document00032.txt', 'suspicious-document00022.txt', 'suspicious-document00001.txt', 'source-document02182.txt', 'source-document04093.txt', 'source-document03582.txt', 'suspicious-document00016.txt', 'suspicious-document00017.txt']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "7021243fd9433fe1a9111124c048cdb5305bebe8"
      },
      "cell_type": "markdown",
      "source": "Below are the functions to clean the text files and combine the files to give dataframes - one each for the source and the suspicious files:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7ec5e6b57cf9c6484acfb0b144d148bcbed6a9b4"
      },
      "cell_type": "code",
      "source": "def clean_file(myfile):\n    mf = myfile.read()\n    mf = mf.lower()\n    mf = re.sub(r'[\\n]\\s*',r' ', mf)\n    mf = re.sub(r'[\\']|[:]|[+]|\\d+|[--]', '', mf)\n    mf = re.sub(r'\\(\\)',r'', mf)\n    mf = re.sub(r'\\.\\s+\\.', r'.', mf)\n    mf = mf.strip()\n    return mf\n\ndef get_dataframe(files):\n    data = []\n    for f in files:\n        with open(path + f, mode='r', encoding='utf-8-sig') as myfile:\n            myfile = clean_file(myfile)\n            data.append(myfile)\n    df = pd.DataFrame(data, columns=['Text'])\n    return df",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "43ee4861ab727bbb00f81c404bb31392ca347cb1"
      },
      "cell_type": "markdown",
      "source": "Next we use the above functions to get a pandas dataframe for the suspicious file. The reason we put the files as a dataframe is because it will be easier to apply the same operations later on to each file."
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "suspicious_files = sorted([f for f in os.listdir(path) if f.startswith('suspicious-document')])\nsuspicious = get_dataframe(suspicious_files)\nsuspicious['File_index'] = [f[19:24] for f in suspicious_files]\nsuspicious.head()",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "                                                Text File_index\n0  bible studies in the life of paul historical a...      00001\n1  my impatience to inhabit the hermitage not per...      00002\n2  morning on the beachthe three letters        i...      00003\n3  this morning it rained so hard (though it was ...      00004\n4  deadham hard a romance by lucas malet (mary st...      00005",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>File_index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>bible studies in the life of paul historical a...</td>\n      <td>00001</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>my impatience to inhabit the hermitage not per...</td>\n      <td>00002</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>morning on the beachthe three letters        i...</td>\n      <td>00003</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>this morning it rained so hard (though it was ...</td>\n      <td>00004</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>deadham hard a romance by lucas malet (mary st...</td>\n      <td>00005</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "1b527987e7e547c46d2c0eb17f1dde23f8f36877"
      },
      "cell_type": "markdown",
      "source": "Similarly, below is the dataframe for the source files:"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "source_files = sorted([f for f in os.listdir(path) if f.startswith('source-document')])\nsource = get_dataframe(source_files)\nsource['File_index'] = [f[15:20] for f in source_files]\nsource.head()",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "                                                Text File_index\n0  mrs. ernest f. wurtele. take a piece of frozen...      00893\n1  after minutely examining every page of the man...      01328\n2  the miscellaneous writings and speeches of lor...      01595\n3  sister teresa by george moore london t. fisher...      01600\n4  i was still wrestling on the pavement with the...      01651",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>File_index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>mrs. ernest f. wurtele. take a piece of frozen...</td>\n      <td>00893</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>after minutely examining every page of the man...</td>\n      <td>01328</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>the miscellaneous writings and speeches of lor...</td>\n      <td>01595</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sister teresa by george moore london t. fisher...</td>\n      <td>01600</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>i was still wrestling on the pavement with the...</td>\n      <td>01651</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "8cf181855d959e5dcf552218b899f5631708721e"
      },
      "cell_type": "markdown",
      "source": "Now, we get trigrams from the corpus of the files so that we can use them to detect plagiarism. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9393d3b44196431572232e8ed2c03f997f54dc11"
      },
      "cell_type": "code",
      "source": "def get_trigrams(df):\n    df['Tokenized_text'] = df['Text'].apply(word_tokenize) \n    df['Trigrams'] = df['Tokenized_text'].apply(lambda x: set(trigrams(x)))\n    return df",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "24be23ea40106ee3ad6aeb477f4865b557e833c1"
      },
      "cell_type": "markdown",
      "source": "Getting trigrams for the suspicious files:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "450ca15271279cc4024d66c45aa9185e8ee897ee"
      },
      "cell_type": "code",
      "source": "suspicious = get_trigrams(suspicious)\nsuspicious.head()",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "                                                Text                        ...                                                                   Trigrams\n0  bible studies in the life of paul historical a...                        ...                          {(still, recognizable, .), (stage, in, the), (...\n1  my impatience to inhabit the hermitage not per...                        ...                          {(findley, ., ''), (resolution, of, seeing), (...\n2  morning on the beachthe three letters        i...                        ...                          {(,, boys, ,), (thinking, i, lay), (,, with, e...\n3  this morning it rained so hard (though it was ...                        ...                          {(dinner, we, fell), (love, to, my), (and, dow...\n4  deadham hard a romance by lucas malet (mary st...                        ...                          {(religion, as, a), (neither, enjoyed, ,), (at...\n\n[5 rows x 4 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>File_index</th>\n      <th>Tokenized_text</th>\n      <th>Trigrams</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>bible studies in the life of paul historical a...</td>\n      <td>00001</td>\n      <td>[bible, studies, in, the, life, of, paul, hist...</td>\n      <td>{(still, recognizable, .), (stage, in, the), (...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>my impatience to inhabit the hermitage not per...</td>\n      <td>00002</td>\n      <td>[my, impatience, to, inhabit, the, hermitage, ...</td>\n      <td>{(findley, ., ''), (resolution, of, seeing), (...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>morning on the beachthe three letters        i...</td>\n      <td>00003</td>\n      <td>[morning, on, the, beachthe, three, letters, i...</td>\n      <td>{(,, boys, ,), (thinking, i, lay), (,, with, e...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>this morning it rained so hard (though it was ...</td>\n      <td>00004</td>\n      <td>[this, morning, it, rained, so, hard, (, thoug...</td>\n      <td>{(dinner, we, fell), (love, to, my), (and, dow...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>deadham hard a romance by lucas malet (mary st...</td>\n      <td>00005</td>\n      <td>[deadham, hard, a, romance, by, lucas, malet, ...</td>\n      <td>{(religion, as, a), (neither, enjoyed, ,), (at...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "309537041cac1f6e5ecee0ab40e6d0e5483d59ab"
      },
      "cell_type": "markdown",
      "source": "Getting trigrams for the source files:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "77677d65552c35eda55d565a817ee4fd09b79079"
      },
      "cell_type": "code",
      "source": "source = get_trigrams(source)\nsource.head()",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "                                                Text                        ...                                                                   Trigrams\n0  mrs. ernest f. wurtele. take a piece of frozen...                        ...                          {(harveys, sauce, and), (., miss, fry), (., pu...\n1  after minutely examining every page of the man...                        ...                          {(them, exactly, in), (in, maintaining, its), ...\n2  the miscellaneous writings and speeches of lor...                        ...                          {(antiquity, ,, liberty), (find, nothing, anal...\n3  sister teresa by george moore london t. fisher...                        ...                          {(get, about, ,), (to, read, some), (about, it...\n4  i was still wrestling on the pavement with the...                        ...                          {(seems, to, recall), (i, actually, heard), (i...\n\n[5 rows x 4 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>File_index</th>\n      <th>Tokenized_text</th>\n      <th>Trigrams</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>mrs. ernest f. wurtele. take a piece of frozen...</td>\n      <td>00893</td>\n      <td>[mrs., ernest, f., wurtele, ., take, a, piece,...</td>\n      <td>{(harveys, sauce, and), (., miss, fry), (., pu...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>after minutely examining every page of the man...</td>\n      <td>01328</td>\n      <td>[after, minutely, examining, every, page, of, ...</td>\n      <td>{(them, exactly, in), (in, maintaining, its), ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>the miscellaneous writings and speeches of lor...</td>\n      <td>01595</td>\n      <td>[the, miscellaneous, writings, and, speeches, ...</td>\n      <td>{(antiquity, ,, liberty), (find, nothing, anal...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sister teresa by george moore london t. fisher...</td>\n      <td>01600</td>\n      <td>[sister, teresa, by, george, moore, london, t....</td>\n      <td>{(get, about, ,), (to, read, some), (about, it...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>i was still wrestling on the pavement with the...</td>\n      <td>01651</td>\n      <td>[i, was, still, wrestling, on, the, pavement, ...</td>\n      <td>{(seems, to, recall), (i, actually, heard), (i...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "830fc9a78087d274edd1d1acb5ade172c06397b1"
      },
      "cell_type": "markdown",
      "source": "Next we compare the suspicious files with the source files using three similarity measures:\n1. Jaccard similarity coefficient\n2. Containment measure\n3. Longest common sequence\n\nThe formulae and explanation for these measures can be found in this [paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.458.9440&rep=rep1&type=pdf)."
    },
    {
      "metadata": {
        "_uuid": "0ec83bde80688ddc062a169b4df416b3f4331676"
      },
      "cell_type": "markdown",
      "source": "We write the code for the three measures."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "456eb633750c9408a5d45acc871b1ae24aa8d434"
      },
      "cell_type": "code",
      "source": "def Jaccard_similarity_coefficient(A, B):\n    J = len(A.intersection(B))/len(A.union(B))\n    return J\n\ndef containment_measure(A, B):\n    J = len(A.intersection(B))/len(B)\n    return J\n\ndef LCS(A, B):\n    m, n = len(A), len(B)\n    counter = [[0]*(n+1) for x in range(m+1)]\n    A, B = list(A), list(B)\n    longest = 0\n    for i in range(m):\n        for j in range(n):\n            if A[i] == B[j]:\n                count = counter[i][j] + 1\n                counter[i+1][j+1] = count\n                if count > longest:\n                    longest = count\n    return longest",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "94c14c90e4c1277fc7332cabd101fa8df7392bed"
      },
      "cell_type": "markdown",
      "source": "We write the functions to apply the above three measures to each suspicious file in the dataframe.  For each suspicious file, we compare it with all source files and keep the highest score for the respective measure. \n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1c666ea1e9967538f5dfd104395fe6dddf4aee72"
      },
      "cell_type": "code",
      "source": "def check_plagiarism_Jaccard(doc_trigrams):\n    Jaccard_similarity_scores = source.Trigrams.apply(lambda s: Jaccard_similarity_coefficient(s, doc_trigrams))\n    most_similar = Jaccard_similarity_scores.idxmax()\n    return Jaccard_similarity_scores[most_similar]#, source.loc[most_similar, 'File_index']\n\ndef check_plagiarism_containment(doc_trigrams):\n    containment_measure_scores = source.Trigrams.apply(lambda s: containment_measure(s, doc_trigrams))\n    most_similar = containment_measure_scores.idxmax()\n    return containment_measure_scores[most_similar]#, source.loc[most_similar, 'File_index']\n\ndef check_plagiarism_LCS(doc_trigrams):\n    LCS_scores = source.Trigrams.apply(lambda s: LCS(s, doc_trigrams))\n    most_similar = LCS_scores.idxmax()\n    return LCS_scores[most_similar]#, source.loc[most_similar, 'File_index']",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "86fedb5b2bcbfb4fd80b9609cc7c05e8272af1ca"
      },
      "cell_type": "markdown",
      "source": "We get the three measures for comparing the similarity between trigrams of suspicious and source files.  "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4c150dd6513e723cbc3f9a67394672d589d713e2"
      },
      "cell_type": "code",
      "source": "suspicious['Jaccard_similarity_score'] = suspicious.Trigrams.apply(check_plagiarism_Jaccard)\nsuspicious['Containment_measure_score'] = suspicious.Trigrams.apply(check_plagiarism_containment)\n# suspicious['Longest_common_sequence'] = suspicious.Trigrams.apply(check_plagiarism_LCS)",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "206ab191599ffe09631bd52eda0d2f934d91a5e7"
      },
      "cell_type": "markdown",
      "source": "The above three measures are targeted to catch the plagiarism where words are more or less copied from the source file. Now we use Latent Semantic Analysis."
    },
    {
      "metadata": {
        "_uuid": "c05f0a98a51ac0fa05f6cc7f333dff2dab8f56d2"
      },
      "cell_type": "markdown",
      "source": " ### Latent Semantic Analysis:\nNext we use scikit-learn's ``TfidfVectorizer`` along with lemmatization to get a document term matrix where the columns corresponds to the files (source and suspicious both)."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f7e23d806f3d0cdd572deb2407b74a70a80c9260"
      },
      "cell_type": "code",
      "source": "class LemmaTokenizer(object):\n    def __init__(self):\n        self.wnl = WordNetLemmatizer()\n    def __call__(self, doc):\n        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n\nvectorizer = TfidfVectorizer(\n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    tokenizer=LemmaTokenizer(),\n    ngram_range=(1, 4),\n    max_features=1000,\n    )\n\nDTM = vectorizer.fit_transform(suspicious.Text.append(source.Text))",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "417ae262a572ac06fa85f58fdbceccd03210fdb0"
      },
      "cell_type": "markdown",
      "source": "Now we use scikit-learn's ``TruncatedSVD`` to apply Singular Value Decomposition on the document term matrix *DTM* obtained above to get a lower dimensional matrix *DTM_LSA* with dim=40 and then normalize it."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cc253f1838357d16b2f38b4d5174151a249b73de"
      },
      "cell_type": "code",
      "source": "LSA = TruncatedSVD(40, algorithm = 'arpack')\nDTM_LSA = LSA.fit_transform(DTM)\nDTM_LSA = Normalizer(copy=False).fit_transform(DTM_LSA)",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cee0f996bf5d74c2599556755db685f1ada22c6c"
      },
      "cell_type": "markdown",
      "source": "Since we have normalized the matrix ``DTM_LSA``,  the dot product of the vectors corresponding to two files will give the cosine angle between them, which is precisely the measure of similarity in this case. Hence, we get the similarity matrix by multiplying the matrix *DTM_LSA* with its transpose."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c068db6257e362baca70d421d601aa2db0d5d398"
      },
      "cell_type": "code",
      "source": "similarity_matrix = np.asarray(np.asmatrix(DTM_LSA) * np.asmatrix(DTM_LSA).T)",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b9016f92211fdb1bc1d2364e1abff3df9ee118f0"
      },
      "cell_type": "markdown",
      "source": "Next we find the highest similarity score for each suspicious document while considering the values corresponding to the source documents only. We achieve this by first setting all the diagonal values as well as the LXL square matrix to zero and taking the max value for each row."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d3756b4cce5938a25b372fd895168b32b41304ac"
      },
      "cell_type": "code",
      "source": "np.fill_diagonal(similarity_matrix, 0)\nL = len(suspicious_files)\nsimilarity_matrix[:L, :L] = np.zeros((L, L))\nsuspicious['LSA_similarity'] = np.max(similarity_matrix, 1)[:L]",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f2304ffc510bc5ad47b03c7db8586ad8aee875ec"
      },
      "cell_type": "markdown",
      "source": "The last step is to use all the similarity measures obtained above as the features for the suspicious documents and train a logistic regression model.  For that we split the suspicious documents into train and test sets and keep only the columns corresponding to the similarity measures."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ed6b015c8e3bfa807409798d1c01469e14009820"
      },
      "cell_type": "code",
      "source": "# suspicious.set_index('File_index', inplace=True)\nsuspicious.reset_index(inplace=True)\nsuspicious = suspicious[['LSA_similarity', 'Jaccard_similarity_score', 'Containment_measure_score']]#, 'Longest_common_sequence']]\ny = pd.Series(np.array([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1]))\nX_train, X_test, y_train, y_test = train_test_split(suspicious, y, test_size=0.20)\nclf = LogisticRegression()\nclf.fit(X_train, y_train)\nclf.score(X_test, y_test)",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 29,
          "data": {
            "text/plain": "0.5714285714285714"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "9ddde9145de71b5b57057fbfdebd687e3f25d331"
      },
      "cell_type": "markdown",
      "source": "\n### References:\n1. [Using Natural Language Processing for Automatic Detection of Plagiarism](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.458.9440&rep=rep1&type=pdf)\n2. [The Role of Natural Language Processing Techniques in Plagiarism Detection](https://prezi.com/yhepkzz-qn76/the-role-of-natural-language-processing-techniques-in-plagiarism-detection/)\n3. [The Influence of Text Pre-processing on Plagiarism Detection](https://pdfs.semanticscholar.org/a47c/1a35e2858da1eb82077b572e538a7b0b7b2d.pdf)\n4. [Dataset](https://www.uni-weimar.de/en/media/chairs/computer-science-department/webis/data/corpus-pan-pc-10/)"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}